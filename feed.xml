<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://loremendez.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://loremendez.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-17T14:34:36+00:00</updated><id>https://loremendez.github.io/feed.xml</id><title type="html">blank</title><subtitle>Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">BLEU Metric</title><link href="https://loremendez.github.io/blog/2024/bleu/" rel="alternate" type="text/html" title="BLEU Metric"/><published>2024-08-08T00:00:00+00:00</published><updated>2024-08-08T00:00:00+00:00</updated><id>https://loremendez.github.io/blog/2024/bleu</id><content type="html" xml:base="https://loremendez.github.io/blog/2024/bleu/"><![CDATA[<p><strong>Meaning:</strong> Bilingual Evaluation Understudy.</p> <p><strong>Measure:</strong> how well a generated text compares to a reference output.</p> <p><strong>Useful:</strong> similar to ROUGE, when the goal is to ensure that as much relevant information as possible from the reference summary is included in the generated summary. (how much of the reference content is captured by the generated summary). This is important in contexts where missing critical information can be more detrimental than including some extraneous information. BLEU is mainly used for translation tasks.</p> <p>BLEU’s output is always a number between 0 and 1. This value indicates how similar the candidate text is to the reference texts, with values closer to 1 representing more similar texts. 1 indicates that the generated text is identical to the reference.</p> <p>BLEU is based on precision.</p> <h2 id="intuition-and-calculation">Intuition and Calculation</h2> <p>This is a good tutorial on how to calculate BLEU step by step: <a href="https://medium.com/nlplanet/two-minutes-nlp-learn-the-bleu-metric-by-examples-df015ca73a86">https://medium.com/nlplanet/two-minutes-nlp-learn-the-bleu-metric-by-examples-df015ca73a86</a></p> <h2 id="observations">Observations</h2> <p>This metric has multiple known limitations: <a href="https://huggingface.co/spaces/evaluate-metric/bleu">(2)</a></p> <ul> <li>BLEU compares overlap in tokens from the predictions and references, instead of comparing meaning. This can lead to <strong>discrepancies between BLEU scores and human ratings.</strong></li> <li><strong>Shorter predicted translations achieve higher scores than longer ones</strong>, simply due to how the score is calculated. A brevity penalty is introduced to attempt to counteract this.</li> <li>BLEU scores are <strong>not comparable</strong> across different datasets, nor are they comparable across different languages.</li> <li><strong>BLEU scores can vary greatly</strong> depending on which parameters are used to generate the scores, especially when different tokenization and normalization techniques are used. It is therefore not possible to compare BLEU scores generated using different parameters, or when these parameters are unknown. For more discussion around this topic, see the following issue.</li> <li>We need to consider different tokenizations and/or preprocessing steps <strong>according to the language</strong>.</li> </ul> <h2 id="thresholds">Thresholds</h2> <p>For these type of metrics, it is difficult to establish a threshold, but we can have some references in mind:</p> <ul> <li> <p><a href="https://aclanthology.org/P02-1040/">The original BLEU paper</a> compares BLEU scores of five different models on the same 500-sentence corpus. These scores ranged from 0.0527 to 0.2571.</p> </li> <li> <p><a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">In the Attention is all you need paper</a>, the transformer architecture got a BLEU score of 0.284 on the WMT 2014 English-to-German translation task, and 0.41 on the WMT 2014 English-to-French translation task.</p> </li> </ul> <h2 id="references">References</h2> <ol> <li><a href="https://medium.com/nlplanet/two-minutes-nlp-learn-the-bleu-metric-by-examples-df015ca73a86">Chiusano, Fabio. 2002. Learn BLEU metric by examples.</a></li> <li><a href="https://huggingface.co/spaces/evaluate-metric/bleu">Hugging Face Implementation.</a></li> <li><a href="https://aclanthology.org/P02-1040/">Papineni et al. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation.</a></li> <li><a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">Vaswani et al. 2017. Attention is all you need.</a></li> </ol>]]></content><author><name></name></author><category term="ai-metrics"/><category term="NLP"/><category term="metrics"/><category term="gen-ai"/><summary type="html"><![CDATA[BLEU Metric]]></summary></entry><entry><title type="html">ROUGE Metrics</title><link href="https://loremendez.github.io/blog/2024/rouge/" rel="alternate" type="text/html" title="ROUGE Metrics"/><published>2024-08-07T00:00:00+00:00</published><updated>2024-08-07T00:00:00+00:00</updated><id>https://loremendez.github.io/blog/2024/rouge</id><content type="html" xml:base="https://loremendez.github.io/blog/2024/rouge/"><![CDATA[<p><strong>Meaning:</strong> Recall-Oriented Understudy for Gisting Evaluation.</p> <p><strong>Measure:</strong> how well a generated text compares to a reference output.</p> <p><strong>Useful:</strong> when the goal is to ensure that as much relevant information as possible from the reference summary is included in the generated summary. (how much of the reference content is captured by the generated summary). This is important in contexts where missing critical information can be more detrimental than including some extraneous information.</p> <h2 id="intuition-and-construction">Intuition and Construction</h2> <p>Imagine you want to assess how a summary (or a generated text) compares to a reference text.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>REFERENCE TEXT: I love to eat ice cream
GENERATED TEXT: I love to eat
</code></pre></div></div> <p>First, we would like to see if key words are included. For which we need to identify the words in each text.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>REFERENCE TEXT WORDS: [I], [love], [to], [eat], [ice], [cream]
GENERATED TEXT WORDS:  [I], [love], [to], [eat]
</code></pre></div></div> <p>Then, we will compute precision and recall scores:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>recall = (# overlapping words)/(# words in the reference) = 4/6 = 0,6667
precision = (# overlapping words)/(# words in the candidate) = 4/4 = 1
</code></pre></div></div> <p>And finally combine them, in an overall score (F1-Score) also known as ROUGE-1 Score:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ROUGE-1 = (2 <span class="ge">* precision *</span> recall)/(precision + recall) = (2 <span class="ge">* 1 *</span> 0,6667)/(1 + 0,6667) = 0,8
</code></pre></div></div> <p>It is called ROUGE-1, because we are comparing words, which are also known as 1-grams (unigrams).</p> <p>If instead of comparing single words, we compare every pair of 2 words, also known as 2- grams (bigrams), then it is called the ROUGE-2.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>REFERENCE TEXT BIGRAMS: [I love], [love to], [to eat], [eat ice], [ice cream]
GENERATED TEXT BIGRAMS:  [I love], [love to], [to eat]

recall = (# overlapping bigrams)/(# bigrams in the reference) = 3/5 = 0,6
precision = (# overlapping bigrams)/(# bigrams in the candidate) = 3/3 = 1
ROUGE-2 = (2 <span class="ge">* precision *</span> recall)/(precision + recall) = (2 <span class="ge">* 1 *</span> 0,6)/(1 + 0,6) = 0,75
</code></pre></div></div> <p>And if we compare N-grams following the same logic, then it is called the ROUGE-N Score.</p> <h3 id="calculation-of-rouge-n">Calculation of ROUGE-N</h3> <ol> <li>Get the N-grams of the reference and the generated answer.</li> <li>Calculate the recall an precision: <ul> <li>recall = (overlapping number of N-grams) / (number of N-grams in the reference)</li> <li>precision = (overlapping number of N-grams) / (number of N-grams in the candidate)</li> </ul> </li> <li>Calculate the F1 score, this is the ROUGE-N: <ul> <li>F1 = (2 * precision * recall) / (precision + recall)</li> </ul> </li> </ol> <p>You can calculate the rouge metrics in python with the library <code class="language-plaintext highlighter-rouge">rouge-score</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">rouge_score</span> <span class="kn">import</span> <span class="n">rouge_scorer</span>

<span class="n">scorer</span> <span class="o">=</span> <span class="n">rouge_scorer</span><span class="p">.</span><span class="nc">RougeScorer</span><span class="p">([</span><span class="sh">'</span><span class="s">rouge1</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">rouge2</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">rougeL</span><span class="sh">'</span><span class="p">],</span> <span class="n">use_stemmer</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">scorer</span><span class="p">.</span><span class="nf">score</span><span class="p">(</span><span class="n">reference</span><span class="p">,</span> <span class="n">gen_answer</span><span class="p">)</span>
</code></pre></div></div> <h2 id="variants">Variants</h2> <p><strong>ROUGE-1:</strong> measures the overlap of unigrams (individual words) between the generated summary and the reference summary. Higher scores indicate better coverage of key concepts.</p> <p><strong>ROUGE-2:</strong> measures the overlap of bigrams (two consecutive words). This score is more stringent and better reflects the fluency and coherence of the generated summary.</p> <p><strong>ROUGE-L:</strong> measures the longest common subsequence, which takes into account sentence-level structure and order.</p> <p><strong>ROUGE-L Sum:</strong></p> <ul> <li>is a variant of ROUGE-L specifically designed to handle multi-document summarization. Evaluates the quality of summaries generated from multiple source documents.</li> <li>instead of calculating the LCS for individual sentences or documents, ROUGE-L Sum aggregates the LCS values across all reference summaries and the generated summary.</li> <li>provides a more comprehensive measure of summary quality by considering the collective content coverage from multiple sources.</li> </ul> <p><strong>ROUGE-W:</strong></p> <ul> <li>introduces a weighted variant of the Longest Common Subsequence (LCS) measure.</li> <li>assigns higher weights to longer LCS matches, thereby emphasizing longer contiguous sequences of words that are shared between the generated summary and the reference summary.</li> <li>useful in tasks where capturing longer sequences of words is crucial for summary coherence and informativeness, such as in technical document summarization or summarizing scientific articles.</li> </ul> <p><strong>ROUGE-S:</strong></p> <ul> <li>introduces the concept of skip-bigrams, which allows for gaps (skips) between words in the matching sequences. This is in contrast to standard n-grams (like unigrams and bigrams), which strictly require consecutive words.</li> <li>less sensitive to minor syntactic differences between the generated summary and the reference summary. It can better capture semantic similarity and paraphrasing in the generated summary.</li> <li>provides a measure of how well the generated summary captures the essential content and structure of the reference summary.</li> <li>useful in tasks where the generated summaries may vary in word order or use paraphrases to convey the same meaning as the reference, such as in summarizing opinionated texts or reviews.</li> </ul> <h2 id="many-observations">(Many) Observations</h2> <ul> <li>ROUGE is <strong>case insensitive</strong>, meaning that upper case letters are treated the same way as lower case letters.</li> <li>ROUGE <strong>favors extractive summarization methods</strong> (which directly copy parts of the source text) over abstractive summarization methods (which generate new sentences), as it measures n-gram overlap.</li> <li>It is impossible to obtain 100% ROUGE scores unless we compare the exact same text. In fact, <strong>there is no definitive upper bound</strong> for ROUGE scores, making very difficult to determine the quality of a summary by using this metric only <a href="https://aclanthology.org/E17-2007.pdf">(4)</a>.</li> <li>Different domains (e.g., news, scientific papers, medical texts) might have <strong>different benchmarks</strong>. It’s essential to look at existing literature and benchmarks within the specific domain to set appropriate thresholds. See the table 1 of <a href="https://aclanthology.org/E17-2007.pdf">(4)</a> as an example, where the summaries of news achieved lower scores than Court docs.</li> <li>When comparing different summarization models, the <strong>relative ROUGE scores are more critical than absolute thresholds</strong>.</li> <li><strong>Consistently higher scores</strong> across multiple ROUGE metrics indicate a better-performing model.</li> <li>While higher scores generally indicate better performance, the interpretation of ROUGE metrics should <strong>consider their limitations</strong>, such as sensitivity to sentence structure and word order.</li> <li>Due to the nature of the different languages, it is possible that sometimes you need to implement <strong>additional pre-processing</strong>. <a href="https://aclanthology.org/2020.lrec-1.821.pdf">(7)</a></li> <li>ROUGE-L is the variant that may have more sense to evaluate RAG. <a href="https://arxiv.org/pdf/2005.11401">(8)</a></li> </ul> <p>Now, let’s take a look into the following table:</p> <table> <thead> <tr> <th style="text-align: left">Generated Text</th> <th style="text-align: center">ROUGE-1</th> <th style="text-align: center">ROUGE-2</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Ice cream is my favorite food ever</td> <td style="text-align: center">0,31</td> <td style="text-align: center">0,18</td> </tr> <tr> <td style="text-align: left">Ice cream is my favorite food</td> <td style="text-align: center">0,33</td> <td style="text-align: center">0,20</td> </tr> <tr> <td style="text-align: left">Ice cream is food</td> <td style="text-align: center">0,40</td> <td style="text-align: center">0,25</td> </tr> <tr> <td style="text-align: left">Ice cream</td> <td style="text-align: center">0,50</td> <td style="text-align: center">0,33</td> </tr> <tr> <td style="text-align: left">Ice</td> <td style="text-align: center">0,29</td> <td style="text-align: center">0,00</td> </tr> <tr> <td style="text-align: left">I hate to eat ice cream</td> <td style="text-align: center"><strong>0,83</strong></td> <td style="text-align: center">0,60</td> </tr> <tr> <td style="text-align: left">I hate to eat</td> <td style="text-align: center">0,60</td> <td style="text-align: center">0,25</td> </tr> <tr> <td style="text-align: left">I love to eat</td> <td style="text-align: center">0,80</td> <td style="text-align: center"><strong>0,75</strong></td> </tr> </tbody> </table> <ul> <li><em>“I hate to eat ice cream”</em> scores the highest ROUGE-1 score, indicating it captures the most critical part of the reference sentence. This is a very concise summary focusing on the core concept, although the meaning is completely the opposite.</li> <li>As the candidate summaries become longer and introduce more words not present in the reference, the ROUGE scores generally decrease, indicating a lower degree of overlap and relevance.</li> <li>The scores suggest that brevity combined with key term retention (e.g., <em>“Ice cream”</em>) results in higher ROUGE scores, especially when the reference sentence is short and focused.</li> </ul> <h2 id="thresholds">Thresholds</h2> <p>In <a href="https://aclanthology.org/E17-2007.pdf">(4)</a>, we can observe optimized ROUGE scores for the three different datasets: duc04 (news), wiki, echr (judgement summary from the european court). Although the ROUGE metrics correlate with Human evaluation <a href="https://aclanthology.org/W04-1013.pdf">(5)</a> <a href="https://aclanthology.org/P08-2051.pdf">(6)</a> , establish a threshold depends on if we want a more abstractive or extractive summary, which of course depends also usually on the type of content we are dealing with <a href="https://aclanthology.org/E17-2007.pdf">(4)</a>.</p> <p>However, as a general guidance, if we take the Document Understanding Conference (DUC) Results 2004 <a href="https://paperswithcode.com/sota/text-summarization-on-duc-2004-task-1">(3)</a> and the two aforementioned papers as a baseline, we could consider the following as good:</p> <ul> <li>ROUGE-1 : (∼ 0.4)</li> <li>ROUGE-2 : (∼ 0.2)</li> <li>ROUGE-L : (∼ 0.3)</li> </ul> <p>For german language see</p> <blockquote class="block-warning"> <h5 id="warning">WARNING</h5> <p>Different domains (e.g., news, scientific papers, medical texts) might have different benchmarks. It’s essential to look at existing literature and benchmarks within the specific domain to set appropriate thresholds. <a href="https://aclanthology.org/E17-2007.pdf">(4)</a></p> </blockquote> <h2 id="references">References</h2> <ol> <li><a href="https://huggingface.co/spaces/evaluate-metric/rouge">Hugging Face Implementation.</a></li> <li><a href="https://github.com/google-research/google-research/tree/master/rouge">Google Implementation.</a></li> <li><a href="https://paperswithcode.com/sota/text-summarization-on-duc-2004-task-1">Text Summarization on DUC 2004.</a></li> <li><a href="https://aclanthology.org/E17-2007.pdf">Schluter, Natalie. 2007. The limits of automatic summarisation according to ROUGE.</a></li> <li><a href="https://aclanthology.org/W04-1013.pdf">Lin, CY. 2004. ROUGE: A Package for Automatic Evaluation of Summaries.</a></li> <li><a href="https://aclanthology.org/P08-2051.pdf">Liu, Feifan. 2008. Correlation between ROUGE and Human Evaluation of Extractive Meeting Summaries.</a></li> <li><a href="https://aclanthology.org/2020.lrec-1.821.pdf">Frefel, Dominik. 2020. Summarization Corpora of Wikipedia Articles.</a></li> <li><a href="https://arxiv.org/pdf/2005.11401">Lewis, Patrick. 2021. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.</a></li> <li><a href="https://aclanthology.org/D15-1044.pdf">Rush, Alexander. 2015. A Neural Attention Model for Sentence Summarization.</a></li> <li><a href="https://arxiv.org/pdf/1704.04368">See, Abigail. 2017. Get To The Point: Summarization with Pointer-Generator Networks.</a></li> </ol>]]></content><author><name></name></author><category term="ai-metrics"/><category term="NLP"/><category term="metrics"/><category term="gen-ai"/><summary type="html"><![CDATA[ROUGE Metrics]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://loremendez.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://loremendez.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://loremendez.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry></feed>